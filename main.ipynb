{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "MGVB-6Mz86os"
      },
      "outputs": [],
      "source": [
        "# Given the dataset at google drive, unzip it to Google Colab to\n",
        "# minimize file access times and improve performance\n",
        "\n",
        "# I seperated this part in another block of code, in case the user doesn't use Colab\n",
        "\n",
        "!pip install zipfile36\n",
        "import zipfile\n",
        "z=zipfile.ZipFile('/content/drive/MyDrive/CV PROJECT/dataset.zip','r')\n",
        "z.extractall('/content/')\n",
        "z.close()\n",
        "\n",
        "# Install RoMa (! is used for Colab)\n",
        "\n",
        "!git clone https://github.com/Parskatt/RoMa\n",
        "%cd RoMa\n",
        "!pip install -r requirements.txt\n",
        "!pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g__B9ulNfd3p",
        "outputId": "93994b07-6ce8-414b-abe0-0e86ccaec5e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Found 18840 pairs in /content/dataset/test.csv.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using coarse resolution (560, 560), and upsample res (864, 864)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Estimating F on test pairs:  26%|██▌       | 4863/18840 [48:58<2:22:20,  1.64it/s]"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import math\n",
        "import csv\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "from warnings import warn\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import numpy as np\n",
        "from typing import Union\n",
        "from romatch.models.matcher import ConvRefiner,CosKernel,GP,Decoder\n",
        "from romatch.models.transformer import Block, TransformerDecoder, MemEffAttention\n",
        "from romatch.models.encoders import *\n",
        "from romatch.utils import get_tuple_transform_ops\n",
        "from romatch.utils.utils import check_rgb, cls_to_flow_refine, check_not_i16\n",
        "\n",
        "# Delete if Colab is not used\n",
        "from google.colab import files\n",
        "\n",
        "\"\"\"\n",
        "Computer-Vision-SfM-relative-pose-estimator\n",
        "Author: Guni Deyo Haness\n",
        "\"\"\"\n",
        "\n",
        "# This class performs regression-based matching using an encoder-decoder architecture.\n",
        "class RegressionMatcher(nn.Module):\n",
        "    # Initialize the matcher with encoder, decoder, and various configuration parameters.\n",
        "    def __init__(\n",
        "        self,\n",
        "        encoder,\n",
        "        decoder,\n",
        "        h=448,\n",
        "        w=448,\n",
        "        sample_mode = \"threshold_balanced\",\n",
        "        upsample_preds = False,\n",
        "        symmetric = False,\n",
        "        name = None,\n",
        "        attenuate_cert = None,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        # Optionally attenuate the certainty scores if needed.\n",
        "        self.attenuate_cert = attenuate_cert\n",
        "        # Encoder for feature extraction from input images.\n",
        "        self.encoder = encoder\n",
        "        # Decoder to compute correspondences from extracted features.\n",
        "        self.decoder = decoder\n",
        "        # Optional name identifier for the matcher instance.\n",
        "        self.name = name\n",
        "        # Store resized image width.\n",
        "        self.w_resized = w\n",
        "        # Store resized image height.\n",
        "        self.h_resized = h\n",
        "        # Set up original image transforms; here only normalization is applied.\n",
        "        self.og_transforms = get_tuple_transform_ops(resize=None, normalize=True)\n",
        "        # Define the sampling strategy (e.g., \"threshold_balanced\").\n",
        "        self.sample_mode = sample_mode\n",
        "        # Flag to decide if predictions should be upsampled.\n",
        "        self.upsample_preds = upsample_preds\n",
        "        # Define the resolution used when upsampling predictions.\n",
        "        self.upsample_res = (14*16*6, 14*16*6)\n",
        "        # Boolean flag to use symmetric matching.\n",
        "        self.symmetric = symmetric\n",
        "        # Threshold value used during sampling (optimized after fine-tuning).\n",
        "        self.sample_thresh = 2.5 # Optimized parameter after fine-tuning (default value in source code is 0.5)\n",
        "\n",
        "    # Returns the output resolution based on whether upsampling is enabled.\n",
        "    def get_output_resolution(self):\n",
        "        if not self.upsample_preds:\n",
        "            return self.h_resized, self.w_resized\n",
        "        else:\n",
        "            return self.upsample_res\n",
        "\n",
        "    # Extracts features from the backbone encoder for both images in the batch.\n",
        "    def extract_backbone_features(self, batch, batched = True, upsample = False):\n",
        "        # Get query image (im_A) and source image (im_B) from the batch.\n",
        "        x_q = batch[\"im_A\"]\n",
        "        x_s = batch[\"im_B\"]\n",
        "        if batched:\n",
        "            # If batched, concatenate along the batch dimension and extract features together.\n",
        "            X = torch.cat((x_q, x_s), dim = 0)\n",
        "            feature_pyramid = self.encoder(X, upsample = upsample)\n",
        "        else:\n",
        "            # Process images separately if not batched.\n",
        "            feature_pyramid = self.encoder(x_q, upsample = upsample), self.encoder(x_s, upsample = upsample)\n",
        "        return feature_pyramid\n",
        "\n",
        "    # A fast kernel density estimation function that computes density via matrix multiplications.\n",
        "    def fast_kde(self,x, std=0.1, half=True, down=None):\n",
        "        \"\"\"\n",
        "        A fast version of KDE that computes the pairwise squared Euclidean distances\n",
        "        using matrix multiplications rather than torch.cdist. This should be faster\n",
        "        than the original if memory permits.\n",
        "\n",
        "        This version computes:\n",
        "\n",
        "            dist_sq = ||x||^2 + ||x2||^2.T - 2 * (x @ x2.T)\n",
        "            scores = exp(-dist_sq / (2*std^2))\n",
        "            density = scores.sum(dim=-1)\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor of shape [N, d].\n",
        "            std (float): Standard deviation for the Gaussian kernel.\n",
        "            half (bool): Whether to convert x to half precision.\n",
        "            down (int or None): If provided, use x[::down] as the second argument.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: A tensor of shape [N] containing the density estimates.\n",
        "        \"\"\"\n",
        "        # Convert tensor to half precision if required.\n",
        "        if half:\n",
        "            x = x.half()\n",
        "\n",
        "        # Choose the second tensor based on the downsampling parameter.\n",
        "        x2 = x[::down] if down is not None else x\n",
        "\n",
        "        # Compute squared norms for each vector.\n",
        "        x_norm = (x ** 2).sum(dim=1, keepdim=True)  # shape [N, 1]\n",
        "        x2_norm = (x2 ** 2).sum(dim=1, keepdim=True)  # shape [M, 1]\n",
        "\n",
        "        # Compute squared Euclidean distances using the formula with broadcasting.\n",
        "        dist_sq = x_norm + x2_norm.T - 2 * (x @ x2.T)\n",
        "        # Clamp negative values (caused by floating point errors) to 0.\n",
        "        dist_sq = torch.clamp(dist_sq, min=0.0)\n",
        "\n",
        "        # Compute Gaussian kernel scores based on the squared distances.\n",
        "        scores = torch.exp(-dist_sq / (2 * std**2))\n",
        "        # Sum the scores to obtain the density estimate for each point.\n",
        "        density = scores.sum(dim=-1)\n",
        "        return density\n",
        "\n",
        "    # Sample matches based on certainty and optionally balance them using KDE.\n",
        "    def sample(self, matches, certainty, num=10000):\n",
        "        # If threshold sampling is enabled, adjust certainty values above the threshold.\n",
        "        if \"threshold\" in self.sample_mode:\n",
        "            upper_thresh = self.sample_thresh\n",
        "            certainty = certainty.clone()\n",
        "            certainty[certainty > upper_thresh] = 1\n",
        "\n",
        "        # Flatten the matches and certainty tensors to 2D and 1D respectively.\n",
        "        matches = matches.reshape(-1, 4)\n",
        "        certainty = certainty.reshape(-1)\n",
        "\n",
        "        # Set expansion factor for balanced sampling mode.\n",
        "        expansion_factor = 4 if \"balanced\" in self.sample_mode else 1\n",
        "        # Use multinomial sampling to select a subset of good samples.\n",
        "        good_samples = torch.multinomial(\n",
        "            certainty,\n",
        "            num_samples=min(expansion_factor * num, len(certainty)),\n",
        "            replacement=False\n",
        "        )\n",
        "        good_matches = matches[good_samples]\n",
        "        good_certainty = certainty[good_samples]\n",
        "\n",
        "        # If not in balanced mode, return the selected matches directly.\n",
        "        if \"balanced\" not in self.sample_mode:\n",
        "            return good_matches, good_certainty\n",
        "\n",
        "        # For balanced mode, compute density estimates using the fast KDE.\n",
        "        density = self.fast_kde(good_matches, std=0.1, half=True, down=None)\n",
        "\n",
        "        # Compute sampling probabilities inversely proportional to density.\n",
        "        p = 1 / (density + 1)\n",
        "        # For low-density areas, set the probability to a very small number.\n",
        "        p[density < 10] = 1e-7\n",
        "\n",
        "        # Resample to obtain balanced samples.\n",
        "        balanced_samples = torch.multinomial(\n",
        "            p,\n",
        "            num_samples=min(num, len(good_certainty)),\n",
        "            replacement=False\n",
        "        )\n",
        "        return good_matches[balanced_samples], good_certainty[balanced_samples]\n",
        "\n",
        "    # Forward pass to compute correspondences between two images.\n",
        "    def forward(self, batch, batched = True, upsample = False, scale_factor = 1):\n",
        "        # Extract feature pyramids for the input batch.\n",
        "        feature_pyramid = self.extract_backbone_features(batch, batched=batched, upsample = upsample)\n",
        "        if batched:\n",
        "            # Split concatenated features into two parts: one for each image.\n",
        "            f_q_pyramid = {\n",
        "                scale: f_scale.chunk(2)[0] for scale, f_scale in feature_pyramid.items()\n",
        "            }\n",
        "            f_s_pyramid = {\n",
        "                scale: f_scale.chunk(2)[1] for scale, f_scale in feature_pyramid.items()\n",
        "            }\n",
        "        else:\n",
        "            f_q_pyramid, f_s_pyramid = feature_pyramid\n",
        "        # Decode the features to generate correspondence predictions.\n",
        "        corresps = self.decoder(f_q_pyramid,\n",
        "                                f_s_pyramid,\n",
        "                                upsample = upsample,\n",
        "                                **(batch[\"corresps\"] if \"corresps\" in batch else {}),\n",
        "                                scale_factor=scale_factor)\n",
        "\n",
        "        return corresps\n",
        "\n",
        "    # Forward pass for symmetric matching where the order of images is swapped.\n",
        "    def forward_symmetric(self, batch, batched = True, upsample = False, scale_factor = 1):\n",
        "        # Extract features for symmetric matching.\n",
        "        feature_pyramid = self.extract_backbone_features(batch, batched = batched, upsample = upsample)\n",
        "        f_q_pyramid = feature_pyramid\n",
        "        # Swap the order of the two chunks for symmetric processing.\n",
        "        f_s_pyramid = {\n",
        "            scale: torch.cat((f_scale.chunk(2)[1], f_scale.chunk(2)[0]), dim = 0)\n",
        "            for scale, f_scale in feature_pyramid.items()\n",
        "        }\n",
        "        # Decode the symmetric features to obtain correspondences.\n",
        "        corresps = self.decoder(f_q_pyramid,\n",
        "                                f_s_pyramid,\n",
        "                                upsample = upsample,\n",
        "                                **(batch[\"corresps\"] if \"corresps\" in batch else {}),\n",
        "                                scale_factor=scale_factor)\n",
        "        return corresps\n",
        "\n",
        "    # Compute confidence from forward-backward consistency of optical flow.\n",
        "    def conf_from_fb_consistency(self, flow_forward, flow_backward, th = 2):\n",
        "        # Assumes flow_forward is of shape (..., H, W, 2)\n",
        "        has_batch = False\n",
        "        # Add batch dimension if missing.\n",
        "        if len(flow_forward.shape) == 3:\n",
        "            flow_forward, flow_backward = flow_forward[None], flow_backward[None]\n",
        "        else:\n",
        "            has_batch = True\n",
        "        H,W = flow_forward.shape[-3:-1]\n",
        "        # Adjust threshold relative to image size.\n",
        "        th_n = 2 * th / max(H,W)\n",
        "        # Generate a grid of normalized coordinates for the image.\n",
        "        coords = torch.stack(torch.meshgrid(\n",
        "            torch.linspace(-1 + 1 / W, 1 - 1 / W, W),\n",
        "            torch.linspace(-1 + 1 / H, 1 - 1 / H, H), indexing = \"xy\"),\n",
        "                             dim = -1).to(flow_forward.device)\n",
        "        # Warp the backward flow using the forward flow field.\n",
        "        coords_fb = F.grid_sample(\n",
        "            flow_backward.permute(0, 3, 1, 2),\n",
        "            flow_forward,\n",
        "            align_corners=False, mode=\"bilinear\").permute(0, 2, 3, 1)\n",
        "        # Compute the Euclidean distance between original and warped coordinates.\n",
        "        diff = (coords - coords_fb).norm(dim=-1)\n",
        "        # Determine which pixels are consistent within the threshold.\n",
        "        in_th = (diff < th_n).float()\n",
        "        if not has_batch:\n",
        "            in_th = in_th[0]\n",
        "        return in_th\n",
        "\n",
        "    # Convert normalized coordinates to pixel coordinates.\n",
        "    def to_pixel_coordinates(self, coords, H_A, W_A, H_B = None, W_B = None):\n",
        "        if coords.shape[-1] == 2:\n",
        "            return self._to_pixel_coordinates(coords, H_A, W_A)\n",
        "\n",
        "        if isinstance(coords, (list, tuple)):\n",
        "            kpts_A, kpts_B = coords[0], coords[1]\n",
        "        else:\n",
        "            kpts_A, kpts_B = coords[...,:2], coords[...,2:]\n",
        "        return self._to_pixel_coordinates(kpts_A, H_A, W_A), self._to_pixel_coordinates(kpts_B, H_B, W_B)\n",
        "\n",
        "    # Helper function to perform the actual conversion.\n",
        "    def _to_pixel_coordinates(self, coords, H, W):\n",
        "        # Scale normalized coordinates from [-1,1] to pixel coordinate space.\n",
        "        kpts = torch.stack((W/2 * (coords[...,0]+1), H/2 * (coords[...,1]+1)),axis=-1)\n",
        "        return kpts\n",
        "\n",
        "    # Convert pixel coordinates to normalized coordinates.\n",
        "    def to_normalized_coordinates(self, coords, H_A, W_A, H_B, W_B):\n",
        "        if isinstance(coords, (list, tuple)):\n",
        "            kpts_A, kpts_B = coords[0], coords[1]\n",
        "        else:\n",
        "            kpts_A, kpts_B = coords[...,:2], coords[...,2:]\n",
        "        kpts_A = torch.stack((2/W_A * kpts_A[...,0] - 1, 2/H_A * kpts_A[...,1] - 1),axis=-1)\n",
        "        kpts_B = torch.stack((2/W_B * kpts_B[...,0] - 1, 2/H_B * kpts_B[...,1] - 1),axis=-1)\n",
        "        return kpts_A, kpts_B\n",
        "\n",
        "    # Match keypoints between two images using grid sampling and distance calculations.\n",
        "    def match_keypoints(self, x_A, x_B, warp, certainty, return_tuple = True, return_inds = False):\n",
        "        # Warp image A keypoints into image B space.\n",
        "        x_A_to_B = F.grid_sample(warp[...,-2:].permute(2,0,1)[None], x_A[None,None], align_corners = False, mode = \"bilinear\")[0,:,0].mT\n",
        "        # Sample certainty values corresponding to the warped keypoints.\n",
        "        cert_A_to_B = F.grid_sample(certainty[None,None,...], x_A[None,None], align_corners = False, mode = \"bilinear\")[0,0,0]\n",
        "        # Compute pairwise Euclidean distances between warped keypoints and keypoints in image B.\n",
        "        D = torch.cdist(x_A_to_B, x_B)\n",
        "        # Find indices where the minimal distance condition holds in both directions and meets the certainty threshold.\n",
        "        inds_A, inds_B = torch.nonzero((D == D.min(dim=-1, keepdim = True).values) * (D == D.min(dim=-2, keepdim = True).values) * (cert_A_to_B[:,None] > self.sample_thresh), as_tuple = True)\n",
        "\n",
        "        # Return either the keypoints or their indices based on the function parameters.\n",
        "        if return_tuple:\n",
        "            if return_inds:\n",
        "                return inds_A, inds_B\n",
        "            else:\n",
        "                return x_A[inds_A], x_B[inds_B]\n",
        "        else:\n",
        "            if return_inds:\n",
        "                return torch.cat((inds_A, inds_B),dim=-1)\n",
        "            else:\n",
        "                return torch.cat((x_A[inds_A], x_B[inds_B]),dim=-1)\n",
        "\n",
        "    # Main matching function with inference mode enabled.\n",
        "    @torch.inference_mode()\n",
        "    def match(\n",
        "        self,\n",
        "        im_A_input,\n",
        "        im_B_input,\n",
        "        *args,\n",
        "        batched=False,\n",
        "        device=None,\n",
        "    ):\n",
        "        # Set device to CUDA if available, otherwise use CPU.\n",
        "        if device is None:\n",
        "            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        # If the input for image A is a file path, load and convert it.\n",
        "        if isinstance(im_A_input, (str, os.PathLike)):\n",
        "            im_A = Image.open(im_A_input)\n",
        "            check_not_i16(im_A)\n",
        "            im_A = im_A.convert(\"RGB\")\n",
        "        else:\n",
        "            # Verify that the provided image is in RGB.\n",
        "            check_rgb(im_A_input)\n",
        "            im_A = im_A_input\n",
        "\n",
        "        # Repeat the same for image B.\n",
        "        if isinstance(im_B_input, (str, os.PathLike)):\n",
        "            im_B = Image.open(im_B_input)\n",
        "            check_not_i16(im_B)\n",
        "            im_B = im_B.convert(\"RGB\")\n",
        "        else:\n",
        "            check_rgb(im_B_input)\n",
        "            im_B = im_B_input\n",
        "\n",
        "        symmetric = self.symmetric\n",
        "        self.train(False)  # Set model to evaluation mode.\n",
        "        with torch.no_grad():\n",
        "            if not batched:\n",
        "                # For single image pairs, resize and normalize images to expected dimensions.\n",
        "                b = 1\n",
        "                w, h = im_A.size\n",
        "                w2, h2 = im_B.size\n",
        "                ws = self.w_resized\n",
        "                hs = self.h_resized\n",
        "\n",
        "                test_transform = get_tuple_transform_ops(\n",
        "                    resize=(hs, ws), normalize=True, clahe=False\n",
        "                )\n",
        "                im_A, im_B = test_transform((im_A, im_B))\n",
        "                batch = {\"im_A\": im_A[None].to(device), \"im_B\": im_B[None].to(device)}\n",
        "            else:\n",
        "                # For batched images, check that both have the same size.\n",
        "                b, c, h, w = im_A.shape\n",
        "                b, c, h2, w2 = im_B.shape\n",
        "                assert w == w2 and h == h2, \"For batched images we assume same size\"\n",
        "                batch = {\"im_A\": im_A.to(device), \"im_B\": im_B.to(device)}\n",
        "                if h != self.h_resized or self.w_resized != w:\n",
        "                    warn(\"Model resolution and batch resolution differ, may produce unexpected results\")\n",
        "                hs, ws = h, w\n",
        "            finest_scale = 1\n",
        "            # Run the matcher using either symmetric or standard forward pass.\n",
        "            if symmetric:\n",
        "                corresps = self.forward_symmetric(batch)\n",
        "            else:\n",
        "                corresps = self.forward(batch, batched=True)\n",
        "\n",
        "            if self.upsample_preds:\n",
        "                hs, ws = self.upsample_res\n",
        "\n",
        "            # Optionally adjust certainty using an attenuation factor.\n",
        "            if self.attenuate_cert:\n",
        "                low_res_certainty = F.interpolate(\n",
        "                    corresps[16][\"certainty\"], size=(hs, ws), align_corners=False, mode=\"bilinear\"\n",
        "                )\n",
        "                cert_clamp = 0\n",
        "                factor = 0.5\n",
        "                low_res_certainty = factor * low_res_certainty * (low_res_certainty < cert_clamp)\n",
        "\n",
        "            if self.upsample_preds:\n",
        "                # Upsample predictions to a finer resolution.\n",
        "                finest_corresps = corresps[finest_scale]\n",
        "                torch.cuda.empty_cache()\n",
        "                test_transform = get_tuple_transform_ops(\n",
        "                    resize=(hs, ws), normalize=True\n",
        "                )\n",
        "                if isinstance(im_A_input, (str, os.PathLike)):\n",
        "                    im_A, im_B = test_transform(\n",
        "                        (Image.open(im_A_input).convert('RGB'), Image.open(im_B_input).convert('RGB')))\n",
        "                else:\n",
        "                    im_A, im_B = test_transform((im_A_input, im_B_input))\n",
        "\n",
        "                im_A, im_B = im_A[None].to(device), im_B[None].to(device)\n",
        "                scale_factor = math.sqrt(self.upsample_res[0] * self.upsample_res[1] / (self.w_resized * self.h_resized))\n",
        "                batch = {\"im_A\": im_A, \"im_B\": im_B, \"corresps\": finest_corresps}\n",
        "                if symmetric:\n",
        "                    corresps = self.forward_symmetric(batch, upsample=True, batched=True, scale_factor=scale_factor)\n",
        "                else:\n",
        "                    corresps = self.forward(batch, batched=True, upsample=True, scale_factor=scale_factor)\n",
        "\n",
        "            # Retrieve flow and certainty outputs from the finest scale.\n",
        "            im_A_to_im_B = corresps[finest_scale][\"flow\"]\n",
        "            certainty = corresps[finest_scale][\"certainty\"] - (low_res_certainty if self.attenuate_cert else 0)\n",
        "            if finest_scale != 1:\n",
        "                # If necessary, interpolate flow and certainty to match target resolution.\n",
        "                im_A_to_im_B = F.interpolate(\n",
        "                    im_A_to_im_B, size=(hs, ws), align_corners=False, mode=\"bilinear\"\n",
        "                )\n",
        "                certainty = F.interpolate(\n",
        "                    certainty, size=(hs, ws), align_corners=False, mode=\"bilinear\"\n",
        "                )\n",
        "            # Rearrange the dimensions of the flow tensor.\n",
        "            im_A_to_im_B = im_A_to_im_B.permute(\n",
        "                0, 2, 3, 1\n",
        "            )\n",
        "            # Create a meshgrid of coordinates for image A.\n",
        "            im_A_coords = torch.meshgrid(\n",
        "                (\n",
        "                    torch.linspace(-1 + 1 / hs, 1 - 1 / hs, hs, device=device),\n",
        "                    torch.linspace(-1 + 1 / ws, 1 - 1 / ws, ws, device=device),\n",
        "                ),\n",
        "                indexing='ij'\n",
        "            )\n",
        "            im_A_coords = torch.stack((im_A_coords[1], im_A_coords[0]))\n",
        "            im_A_coords = im_A_coords[None].expand(b, 2, hs, ws)\n",
        "            # Convert certainty logits to probabilities.\n",
        "            certainty = certainty.sigmoid()  # logits -> probs\n",
        "            im_A_coords = im_A_coords.permute(0, 2, 3, 1)\n",
        "            # If the predicted flow is out of range, zero out the corresponding certainty.\n",
        "            if (im_A_to_im_B.abs() > 1).any() and True:\n",
        "                wrong = (im_A_to_im_B.abs() > 1).sum(dim=-1) > 0\n",
        "                certainty[wrong[:, None]] = 0\n",
        "            # Clamp flow values to be within the valid range.\n",
        "            im_A_to_im_B = torch.clamp(im_A_to_im_B, -1, 1)\n",
        "            if symmetric:\n",
        "                # For symmetric matching, split the flow into two components.\n",
        "                A_to_B, B_to_A = im_A_to_im_B.chunk(2)\n",
        "                q_warp = torch.cat((im_A_coords, A_to_B), dim=-1)\n",
        "                im_B_coords = im_A_coords\n",
        "                s_warp = torch.cat((B_to_A, im_B_coords), dim=-1)\n",
        "                warp = torch.cat((q_warp, s_warp), dim=2)\n",
        "                certainty = torch.cat(certainty.chunk(2), dim=3)\n",
        "            else:\n",
        "                # For standard matching, concatenate image A coordinates with flow.\n",
        "                warp = torch.cat((im_A_coords, im_A_to_im_B), dim=-1)\n",
        "            if batched:\n",
        "                return (\n",
        "                    warp,\n",
        "                    certainty[:, 0]\n",
        "                )\n",
        "            else:\n",
        "                return (\n",
        "                    warp[0],\n",
        "                    certainty[0, 0],\n",
        "                )\n",
        "\n",
        "    # Visualize the warp result by overlaying warped images with a certainty map.\n",
        "    def visualize_warp(self, warp, certainty, im_A = None, im_B = None,\n",
        "                       im_A_path = None, im_B_path = None, device = \"cuda\", symmetric = True, save_path = None, unnormalize = False):\n",
        "        #assert symmetric == True, \"Currently assuming bidirectional warp, might update this if someone complains ;)\"\n",
        "        # Determine height and width based on warp dimensions.\n",
        "        H,W2,_ = warp.shape\n",
        "        W = W2//2 if symmetric else W2\n",
        "        # If images are not provided, load them from the given file paths.\n",
        "        if im_A is None:\n",
        "            from PIL import Image\n",
        "            im_A, im_B = Image.open(im_A_path).convert(\"RGB\"), Image.open(im_B_path).convert(\"RGB\")\n",
        "        if not isinstance(im_A, torch.Tensor):\n",
        "            # Resize and convert images to tensors.\n",
        "            im_A = im_A.resize((W,H))\n",
        "            im_B = im_B.resize((W,H))\n",
        "            x_B = (torch.tensor(np.array(im_B)) / 255).to(device).permute(2, 0, 1)\n",
        "            if symmetric:\n",
        "                x_A = (torch.tensor(np.array(im_A)) / 255).to(device).permute(2, 0, 1)\n",
        "        else:\n",
        "            if symmetric:\n",
        "                x_A = im_A\n",
        "            x_B = im_B\n",
        "        # Warp image B using the predicted flow.\n",
        "        im_A_transfer_rgb = F.grid_sample(\n",
        "        x_B[None], warp[:,:W, 2:][None], mode=\"bilinear\", align_corners=False\n",
        "        )[0]\n",
        "        if symmetric:\n",
        "            # Warp image A for symmetric visualization.\n",
        "            im_B_transfer_rgb = F.grid_sample(\n",
        "            x_A[None], warp[:, W:, :2][None], mode=\"bilinear\", align_corners=False\n",
        "            )[0]\n",
        "            # Concatenate the warped images side by side.\n",
        "            warp_im = torch.cat((im_A_transfer_rgb,im_B_transfer_rgb),dim=2)\n",
        "            white_im = torch.ones((H,2*W),device=device)\n",
        "        else:\n",
        "            warp_im = im_A_transfer_rgb\n",
        "            white_im = torch.ones((H, W), device = device)\n",
        "        # Blend the warped image with a white background based on the certainty map.\n",
        "        vis_im = certainty * warp_im + (1 - certainty) * white_im\n",
        "        if save_path is not None:\n",
        "            from romatch.utils import tensor_to_pil\n",
        "            tensor_to_pil(vis_im, unnormalize=unnormalize).save(save_path)\n",
        "        return vis_im\n",
        "\n",
        "\n",
        "# Create the complete Roma model by assembling encoder, decoder, and additional modules.\n",
        "def roma_model(resolution, upsample_preds, device = None, weights=None, dinov2_weights=None, amp_dtype: torch.dtype=torch.float16, **kwargs):\n",
        "    # Suppress specific warnings related to deprecated storage types.\n",
        "    warnings.filterwarnings('ignore', category=UserWarning, message='TypedStorage is deprecated')\n",
        "    # Define dimensions for global projection and features.\n",
        "    gp_dim = 512\n",
        "    feat_dim = 512\n",
        "    decoder_dim = gp_dim + feat_dim\n",
        "    cls_to_coord_res = 64\n",
        "    # Build a transformer-based coordinate decoder.\n",
        "    coordinate_decoder = TransformerDecoder(\n",
        "        nn.Sequential(*[Block(decoder_dim, 8, attn_class=MemEffAttention) for _ in range(5)]),\n",
        "        decoder_dim,\n",
        "        cls_to_coord_res**2 + 1,\n",
        "        is_classifier=True,\n",
        "        amp = True,\n",
        "        pos_enc = False,)\n",
        "    dw = True\n",
        "    hidden_blocks = 8\n",
        "    kernel_size = 5\n",
        "    displacement_emb = \"linear\"\n",
        "    disable_local_corr_grad = True\n",
        "\n",
        "    # Construct convolutional refiners for various scales.\n",
        "    conv_refiner = nn.ModuleDict(\n",
        "        {\n",
        "            \"16\": ConvRefiner(\n",
        "                2 * 512+128+(2*7+1)**2,\n",
        "                2 * 512+128+(2*7+1)**2,\n",
        "                2 + 1,\n",
        "                kernel_size=kernel_size,\n",
        "                dw=dw,\n",
        "                hidden_blocks=hidden_blocks,\n",
        "                displacement_emb=displacement_emb,\n",
        "                displacement_emb_dim=128,\n",
        "                local_corr_radius = 7,\n",
        "                corr_in_other = True,\n",
        "                amp = True,\n",
        "                disable_local_corr_grad = disable_local_corr_grad,\n",
        "                bn_momentum = 0.01,\n",
        "            ),\n",
        "            \"8\": ConvRefiner(\n",
        "                2 * 512+64+(2*3+1)**2,\n",
        "                2 * 512+64+(2*3+1)**2,\n",
        "                2 + 1,\n",
        "                kernel_size=kernel_size,\n",
        "                dw=dw,\n",
        "                hidden_blocks=hidden_blocks,\n",
        "                displacement_emb=displacement_emb,\n",
        "                displacement_emb_dim=64,\n",
        "                local_corr_radius = 3,\n",
        "                corr_in_other = True,\n",
        "                amp = True,\n",
        "                disable_local_corr_grad = disable_local_corr_grad,\n",
        "                bn_momentum = 0.01,\n",
        "            ),\n",
        "            \"4\": ConvRefiner(\n",
        "                2 * 256+32+(2*2+1)**2,\n",
        "                2 * 256+32+(2*2+1)**2,\n",
        "                2 + 1,\n",
        "                kernel_size=kernel_size,\n",
        "                dw=dw,\n",
        "                hidden_blocks=hidden_blocks,\n",
        "                displacement_emb=displacement_emb,\n",
        "                displacement_emb_dim=32,\n",
        "                local_corr_radius = 2,\n",
        "                corr_in_other = True,\n",
        "                amp = True,\n",
        "                disable_local_corr_grad = disable_local_corr_grad,\n",
        "                bn_momentum = 0.01,\n",
        "            ),\n",
        "            \"2\": ConvRefiner(\n",
        "                2 * 64+16,\n",
        "                128+16,\n",
        "                2 + 1,\n",
        "                kernel_size=kernel_size,\n",
        "                dw=dw,\n",
        "                hidden_blocks=hidden_blocks,\n",
        "                displacement_emb=displacement_emb,\n",
        "                displacement_emb_dim=16,\n",
        "                amp = True,\n",
        "                disable_local_corr_grad = disable_local_corr_grad,\n",
        "                bn_momentum = 0.01,\n",
        "            ),\n",
        "            \"1\": ConvRefiner(\n",
        "                2 * 9 + 6,\n",
        "                24,\n",
        "                2 + 1,\n",
        "                kernel_size=kernel_size,\n",
        "                dw=dw,\n",
        "                hidden_blocks = hidden_blocks,\n",
        "                displacement_emb = displacement_emb,\n",
        "                displacement_emb_dim = 6,\n",
        "                amp = True,\n",
        "                disable_local_corr_grad = disable_local_corr_grad,\n",
        "                bn_momentum = 0.01,\n",
        "            ),\n",
        "        }\n",
        "    )\n",
        "    kernel_temperature = 0.2\n",
        "    learn_temperature = False\n",
        "    no_cov = True\n",
        "    kernel = CosKernel\n",
        "    only_attention = False\n",
        "    basis = \"fourier\"\n",
        "    # Create a Gaussian Process module for scale \"16\".\n",
        "    gp16 = GP(\n",
        "        kernel,\n",
        "        T=kernel_temperature,\n",
        "        learn_temperature=learn_temperature,\n",
        "        only_attention=only_attention,\n",
        "        gp_dim=gp_dim,\n",
        "        basis=basis,\n",
        "        no_cov=no_cov,\n",
        "    )\n",
        "    gps = nn.ModuleDict({\"16\": gp16})\n",
        "    # Define projection layers for various scales.\n",
        "    proj16 = nn.Sequential(nn.Conv2d(1024, 512, 1, 1), nn.BatchNorm2d(512))\n",
        "    proj8 = nn.Sequential(nn.Conv2d(512, 512, 1, 1), nn.BatchNorm2d(512))\n",
        "    proj4 = nn.Sequential(nn.Conv2d(256, 256, 1, 1), nn.BatchNorm2d(256))\n",
        "    proj2 = nn.Sequential(nn.Conv2d(128, 64, 1, 1), nn.BatchNorm2d(64))\n",
        "    proj1 = nn.Sequential(nn.Conv2d(64, 9, 1, 1), nn.BatchNorm2d(9))\n",
        "    proj = nn.ModuleDict({\n",
        "        \"16\": proj16,\n",
        "        \"8\": proj8,\n",
        "        \"4\": proj4,\n",
        "        \"2\": proj2,\n",
        "        \"1\": proj1,\n",
        "        })\n",
        "    displacement_dropout_p = 0.0\n",
        "    gm_warp_dropout_p = 0.0\n",
        "    # Assemble the decoder using the coordinate decoder, GP modules, projection layers, and convolutional refiners.\n",
        "    decoder = Decoder(coordinate_decoder,\n",
        "                      gps,\n",
        "                      proj,\n",
        "                      conv_refiner,\n",
        "                      detach=True,\n",
        "                      scales=[\"16\", \"8\", \"4\", \"2\", \"1\"],\n",
        "                      displacement_dropout_p = displacement_dropout_p,\n",
        "                      gm_warp_dropout_p = gm_warp_dropout_p)\n",
        "\n",
        "    # Initialize the encoder using CNN and Dinov2 modules.\n",
        "    encoder = CNNandDinov2(\n",
        "        cnn_kwargs = dict(\n",
        "            pretrained=False,\n",
        "            amp = True),\n",
        "        amp = True,\n",
        "        use_vgg = True,\n",
        "        dinov2_weights = dinov2_weights,\n",
        "        amp_dtype=amp_dtype,\n",
        "    )\n",
        "    h,w = resolution\n",
        "    symmetric = True\n",
        "    attenuate_cert = True\n",
        "    sample_mode = \"threshold_balanced\"\n",
        "    # Create an instance of RegressionMatcher with all the assembled components.\n",
        "    matcher = RegressionMatcher(encoder, decoder, h=h, w=w, upsample_preds=upsample_preds,\n",
        "                                symmetric = symmetric, attenuate_cert = attenuate_cert, sample_mode = sample_mode, **kwargs).to(device)\n",
        "    matcher.load_state_dict(weights)\n",
        "    return matcher\n",
        "\n",
        "\n",
        "# Dictionary containing URLs for pretrained model weights.\n",
        "weight_urls = {\n",
        "    \"romatch\": {\n",
        "        \"outdoor\": \"https://github.com/Parskatt/storage/releases/download/roma/roma_outdoor.pth\",\n",
        "        \"indoor\": \"https://github.com/Parskatt/storage/releases/download/roma/roma_indoor.pth\",\n",
        "    },\n",
        "    \"tiny_roma_v1\": {\n",
        "        \"outdoor\": \"https://github.com/Parskatt/storage/releases/download/roma/tiny_roma_v1_outdoor.pth\",\n",
        "    },\n",
        "    \"dinov2\": \"https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_pretrain.pth\", #hopefully this doesnt change :D\n",
        "}\n",
        "\n",
        "\n",
        "# Convenience function to load and configure the Roma model for outdoor scenes.\n",
        "def roma_outdoor(device, weights=None, dinov2_weights=None, coarse_res: Union[int,tuple[int,int]] = 560, upsample_res: Union[int,tuple[int,int]] = 864, amp_dtype: torch.dtype = torch.float16):\n",
        "    # Convert resolutions to tuples if they are provided as integers.\n",
        "    if isinstance(coarse_res, int):\n",
        "        coarse_res = (coarse_res, coarse_res)\n",
        "    if isinstance(upsample_res, int):\n",
        "        upsample_res = (upsample_res, upsample_res)\n",
        "\n",
        "    # Use float32 precision if running on CPU.\n",
        "    if str(device) == 'cpu':\n",
        "        amp_dtype = torch.float32\n",
        "\n",
        "    # Ensure the coarse resolution dimensions are multiples of 14 as required by the backbone.\n",
        "    assert coarse_res[0] % 14 == 0, \"Needs to be multiple of 14 for backbone\"\n",
        "    assert coarse_res[1] % 14 == 0, \"Needs to be multiple of 14 for backbone\"\n",
        "\n",
        "    # Load model weights from URL if not provided.\n",
        "    if weights is None:\n",
        "        weights = torch.hub.load_state_dict_from_url(weight_urls[\"romatch\"][\"outdoor\"],\n",
        "                                                     map_location=device)\n",
        "    if dinov2_weights is None:\n",
        "        dinov2_weights = torch.hub.load_state_dict_from_url(weight_urls[\"dinov2\"],\n",
        "                                                     map_location=device)\n",
        "    # Build the Roma matcher model.\n",
        "    model = roma_model(resolution=coarse_res, upsample_preds=True,\n",
        "               weights=weights,dinov2_weights = dinov2_weights,device=device, amp_dtype=amp_dtype)\n",
        "    # Set the upsample resolution.\n",
        "    model.upsample_res = upsample_res\n",
        "    print(f\"Using coarse resolution {coarse_res}, and upsample res {model.upsample_res}\")\n",
        "    return model\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "    # Speed up PyTorch's conv algorithms if input sizes are consistent\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    TEST_DIR = \"/content/dataset/test_images\"\n",
        "    TEST_CSV = \"/content/dataset/test.csv\"\n",
        "    SUBMISSION_CSV = \"/content/submission.csv\"\n",
        "    WEIGHTS_PTH = \"/content/roma_full_weights.pth\"\n",
        "\n",
        "    if not os.path.isfile(TEST_CSV) or not os.path.isdir(TEST_DIR):\n",
        "        print(f\"Test Files not found!\")\n",
        "        return\n",
        "\n",
        "    test_rows = []\n",
        "    with open(TEST_CSV, \"r\", newline=\"\") as f:\n",
        "        reader = csv.DictReader(f)\n",
        "        for row in reader:\n",
        "            sample_id = row[\"sample_id\"]\n",
        "            batch_id  = row[\"batch_id\"]\n",
        "            im1_id    = row[\"image_1_id\"]\n",
        "            im2_id    = row[\"image_2_id\"]\n",
        "            test_rows.append((sample_id, batch_id, im1_id, im2_id))\n",
        "    print(f\"Found {len(test_rows)} pairs in {TEST_CSV}.\")\n",
        "\n",
        "    roma_model = roma_outdoor(device=device)\n",
        "\n",
        "    results = []\n",
        "    start_inference = time.time()\n",
        "\n",
        "    try:\n",
        "        # Use inference_mode for speed\n",
        "        with torch.inference_mode():\n",
        "            with tqdm(total=len(test_rows), desc=\"Estimating F on test pairs\") as pbar:\n",
        "                for (sample_id, scene_name, im1_id, im2_id) in test_rows:\n",
        "                    try:\n",
        "                        img1_path = os.path.join(TEST_DIR, scene_name, im1_id + \".jpg\")\n",
        "                        img2_path = os.path.join(TEST_DIR, scene_name, im2_id + \".jpg\")\n",
        "\n",
        "                        # Check existence\n",
        "                        if not (os.path.isfile(img1_path) and os.path.isfile(img2_path)):\n",
        "                            F_est = np.zeros((3,3), dtype=np.float64)\n",
        "                        else:\n",
        "                            # Load sizes\n",
        "                            W_A, H_A = Image.open(img1_path).size\n",
        "                            W_B, H_B = Image.open(img2_path).size\n",
        "\n",
        "                            # Match with RoMa\n",
        "                            warp, certainty = roma_model.match(img1_path, img2_path, device=device)\n",
        "\n",
        "                            # Sample\n",
        "                            matches, c = roma_model.sample(warp, certainty)\n",
        "\n",
        "                            # Convert to pixel coords\n",
        "                            kpts1, kpts2 = roma_model.to_pixel_coordinates(matches, H_A, W_A, H_B, W_B)\n",
        "                            kpts1_np = kpts1.cpu().numpy()\n",
        "                            kpts2_np = kpts2.cpu().numpy()\n",
        "\n",
        "                            if kpts1_np.shape[0] < 8:\n",
        "                                # Not enough matches\n",
        "                                F_est = np.zeros((3,3), dtype=np.float64)\n",
        "\n",
        "                            else:\n",
        "                                # 4) Estimate F with MAGSAC\n",
        "                                '''\n",
        "                                MAGSAC is a RANSAC variation. I'll shortly explain how RANSAC works to find F: Given at least 8 point correspondences,\n",
        "                                8 points will be used to calculate the hypothesis F with SVD, the other correspondences will be checked to be inliers this way: \n",
        "                                given one correspondence (x,x') we will use F to get the epipolar line in which x' lies which should be Fx.\n",
        "                                If the distance from x' to Fx is less than a specified threshold, the correspondence is an inlier.\n",
        "                                After some iterations or when we reached a specified threshold of inliers for an hypothesis, \n",
        "                                we will return the F matrix (model) with the maximal amount of inliers.\n",
        "                                '''\n",
        "                                try:\n",
        "                                    F_est, mask = cv2.findFundamentalMat(\n",
        "                                        kpts1_np,\n",
        "                                        kpts2_np,\n",
        "                                        ransacReprojThreshold=0.7, # Changed from 0.2 after fine-tuning and optimization\n",
        "                                        method=cv2.USAC_MAGSAC,\n",
        "                                        confidence=0.999999,\n",
        "                                        maxIters=10000\n",
        "                                    )\n",
        "                                except cv2.error:\n",
        "                                    F_est = None\n",
        "                                if F_est is None or F_est.shape != (3,3):\n",
        "                                    F_est = np.zeros((3,3), dtype=np.float64)\n",
        "\n",
        "                        # Flatten for submission\n",
        "                        F_str = \" \".join(f\"{val:e}\" for val in F_est.flatten())\n",
        "                        results.append((sample_id, F_str))\n",
        "\n",
        "                    except Exception as e: # Usually happens when VRAM is full\n",
        "                        print(f\"Error processing pair {sample_id}: {e}\")\n",
        "                        F_est = np.zeros((3,3), dtype=np.float64)\n",
        "                        F_str = \" \".join(f\"{val:e}\" for val in F_est.flatten())\n",
        "                        results.append((sample_id, F_str))\n",
        "\n",
        "                    finally:\n",
        "                        pbar.update(1)\n",
        "\n",
        "        end_inference = time.time()\n",
        "        print(f\"Done estimating F for all pairs in {end_inference - start_inference:.2f} seconds.\")\n",
        "\n",
        "        # Save F predictions and model weights\n",
        "\n",
        "        with open(SUBMISSION_CSV, \"w\", newline=\"\") as fout:\n",
        "            writer = csv.writer(fout)\n",
        "            writer.writerow([\"sample_id\", \"fundamental_matrix\"])\n",
        "            for sample_id, F_str in results:\n",
        "                writer.writerow([sample_id, F_str])\n",
        "\n",
        "        print(f\"Wrote {len(results)} rows to {SUBMISSION_CSV}.\")\n",
        "        torch.save(roma_model.state_dict(), WEIGHTS_PTH)\n",
        "        print('Saved mode weights')\n",
        "        files.download(SUBMISSION_CSV) # Remove if code is not run in Colab\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
