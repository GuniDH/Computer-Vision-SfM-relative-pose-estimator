{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "MGVB-6Mz86os"
      },
      "outputs": [],
      "source": [
        "# Given the dataset at google drive, unzip it to Google Colab to\n",
        "# minimize file access times and improve performance\n",
        "\n",
        "# I seperated this part in another block of code, in case the user doesn't use Colab\n",
        "\n",
        "!pip install zipfile36\n",
        "import zipfile\n",
        "z=zipfile.ZipFile('/content/drive/MyDrive/CV PROJECT/dataset.zip','r')\n",
        "z.extractall('/content/')\n",
        "z.close()\n",
        "\n",
        "# Install RoMa (! is used for Colab)\n",
        "\n",
        "!git clone https://github.com/Parskatt/RoMa\n",
        "%cd RoMa\n",
        "!pip install -r requirements.txt\n",
        "!pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g__B9ulNfd3p",
        "outputId": "93994b07-6ce8-414b-abe0-0e86ccaec5e7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import csv\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "from warnings import warn\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import numpy as np\n",
        "from typing import Union\n",
        "\n",
        "from romatch.models.matcher import ConvRefiner,CosKernel,GP,Decoder\n",
        "from romatch.models.transformer import Block, TransformerDecoder, MemEffAttention\n",
        "from romatch.models.encoders import *\n",
        "from romatch.utils import get_tuple_transform_ops\n",
        "from romatch.utils.utils import check_rgb, cls_to_flow_refine, check_not_i16\n",
        "# from romatch.utils.kde import kde\n",
        "# No need to import this because we use our optimized version of kde\n",
        "\n",
        "# Delete if Colab is not used\n",
        "from google.colab import files\n",
        "\n",
        "\"\"\"\n",
        "Computer-Vision-SfM-relative-pose-estimator\n",
        "Author: Guni Deyo Haness\n",
        "\"\"\"\n",
        "\n",
        "# Modified RoMa code",
        "class RegressionMatcher(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        encoder,\n",
        "        decoder,\n",
        "        h=448,\n",
        "        w=448,\n",
        "        sample_mode = \"threshold_balanced\",\n",
        "        upsample_preds = False,\n",
        "        symmetric = False,\n",
        "        name = None,\n",
        "        attenuate_cert = None,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.attenuate_cert = attenuate_cert\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.name = name\n",
        "        self.w_resized = w\n",
        "        self.h_resized = h\n",
        "        self.og_transforms = get_tuple_transform_ops(resize=None, normalize=True)\n",
        "        self.sample_mode = sample_mode\n",
        "        self.upsample_preds = upsample_preds\n",
        "        self.upsample_res = (14*16*6, 14*16*6)\n",
        "        self.symmetric = symmetric\n",
        "        self.sample_thresh = 2.5 # Optimized parameter after fine-tuning (default value in source code is 0.5)\n",
        "\n",
        "    def get_output_resolution(self):\n",
        "        if not self.upsample_preds:\n",
        "            return self.h_resized, self.w_resized\n",
        "        else:\n",
        "            return self.upsample_res\n",
        "\n",
        "    def extract_backbone_features(self, batch, batched = True, upsample = False):\n",
        "        x_q = batch[\"im_A\"]\n",
        "        x_s = batch[\"im_B\"]\n",
        "        if batched:\n",
        "            X = torch.cat((x_q, x_s), dim = 0)\n",
        "            feature_pyramid = self.encoder(X, upsample = upsample)\n",
        "        else:\n",
        "            feature_pyramid = self.encoder(x_q, upsample = upsample), self.encoder(x_s, upsample = upsample)\n",
        "        return feature_pyramid\n",
        "\n",
        "    def fast_kde(self,x, std=0.1, half=True, down=None):\n",
        "        \"\"\"\n",
        "        A fast version of KDE that computes the pairwise squared Euclidean distances\n",
        "        using matrix multiplications rather than torch.cdist. This should be faster\n",
        "        than the original if memory permits.\n",
        "\n",
        "        This version computes:\n",
        "\n",
        "            dist_sq = ||x||^2 + ||x2||^2.T - 2 * (x @ x2.T)\n",
        "            scores = exp(-dist_sq / (2*std^2))\n",
        "            density = scores.sum(dim=-1)\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor of shape [N, d].\n",
        "            std (float): Standard deviation for the Gaussian kernel.\n",
        "            half (bool): Whether to convert x to half precision.\n",
        "            down (int or None): If provided, use x[::down] as the second argument.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: A tensor of shape [N] containing the density estimates.\n",
        "        \"\"\"\n",
        "        if half:\n",
        "            x = x.half()\n",
        "\n",
        "        # Choose second tensor\n",
        "        x2 = x[::down] if down is not None else x\n",
        "\n",
        "        # Compute squared norms\n",
        "        x_norm = (x ** 2).sum(dim=1, keepdim=True)  # shape [N, 1]\n",
        "        x2_norm = (x2 ** 2).sum(dim=1, keepdim=True)  # shape [M, 1]\n",
        "\n",
        "        # Compute squared Euclidean distances:\n",
        "        # dist_sq[i, j] = ||x[i]||^2 + ||x2[j]||^2 - 2*x[i]Â·x2[j]\n",
        "        # We compute this using broadcasting.\n",
        "        dist_sq = x_norm + x2_norm.T - 2 * (x @ x2.T)\n",
        "        # Clamp any negative values (due to floating-point errors) to 0.\n",
        "        dist_sq = torch.clamp(dist_sq, min=0.0)\n",
        "\n",
        "        # Compute Gaussian kernel scores.\n",
        "        scores = torch.exp(-dist_sq / (2 * std**2))\n",
        "        # Sum scores along the second dimension to yield density for each row.\n",
        "        density = scores.sum(dim=-1)\n",
        "        return density\n",
        "\n",
        "    # Modified sample function to use fast_kde\n",
        "    def sample(self, matches, certainty, num=10000):\n",
        "        if \"threshold\" in self.sample_mode:\n",
        "            upper_thresh = self.sample_thresh\n",
        "            certainty = certainty.clone()\n",
        "            certainty[certainty > upper_thresh] = 1\n",
        "\n",
        "        # Flatten matches and certainty\n",
        "        matches = matches.reshape(-1, 4)\n",
        "        certainty = certainty.reshape(-1)\n",
        "\n",
        "        expansion_factor = 4 if \"balanced\" in self.sample_mode else 1\n",
        "        good_samples = torch.multinomial(\n",
        "            certainty,\n",
        "            num_samples=min(expansion_factor * num, len(certainty)),\n",
        "            replacement=False\n",
        "        )\n",
        "        good_matches = matches[good_samples]\n",
        "        good_certainty = certainty[good_samples]\n",
        "\n",
        "        if \"balanced\" not in self.sample_mode:\n",
        "            return good_matches, good_certainty\n",
        "\n",
        "        # Use the fast_kde instead of the original:\n",
        "        density = self.fast_kde(good_matches, std=0.1, half=True, down=None)\n",
        "\n",
        "        p = 1 / (density + 1)\n",
        "        p[density < 10] = 1e-7\n",
        "\n",
        "        balanced_samples = torch.multinomial(\n",
        "            p,\n",
        "            num_samples=min(num, len(good_certainty)),\n",
        "            replacement=False\n",
        "        )\n",
        "        return good_matches[balanced_samples], good_certainty[balanced_samples]\n",
        "\n",
        "    def forward(self, batch, batched = True, upsample = False, scale_factor = 1):\n",
        "        feature_pyramid = self.extract_backbone_features(batch, batched=batched, upsample = upsample)\n",
        "        if batched:\n",
        "            f_q_pyramid = {\n",
        "                scale: f_scale.chunk(2)[0] for scale, f_scale in feature_pyramid.items()\n",
        "            }\n",
        "            f_s_pyramid = {\n",
        "                scale: f_scale.chunk(2)[1] for scale, f_scale in feature_pyramid.items()\n",
        "            }\n",
        "        else:\n",
        "            f_q_pyramid, f_s_pyramid = feature_pyramid\n",
        "        corresps = self.decoder(f_q_pyramid,\n",
        "                                f_s_pyramid,\n",
        "                                upsample = upsample,\n",
        "                                **(batch[\"corresps\"] if \"corresps\" in batch else {}),\n",
        "                                scale_factor=scale_factor)\n",
        "\n",
        "        return corresps\n",
        "\n",
        "    def forward_symmetric(self, batch, batched = True, upsample = False, scale_factor = 1):\n",
        "        feature_pyramid = self.extract_backbone_features(batch, batched = batched, upsample = upsample)\n",
        "        f_q_pyramid = feature_pyramid\n",
        "        f_s_pyramid = {\n",
        "            scale: torch.cat((f_scale.chunk(2)[1], f_scale.chunk(2)[0]), dim = 0)\n",
        "            for scale, f_scale in feature_pyramid.items()\n",
        "        }\n",
        "        corresps = self.decoder(f_q_pyramid,\n",
        "                                f_s_pyramid,\n",
        "                                upsample = upsample,\n",
        "                                **(batch[\"corresps\"] if \"corresps\" in batch else {}),\n",
        "                                scale_factor=scale_factor)\n",
        "        return corresps\n",
        "\n",
        "    def conf_from_fb_consistency(self, flow_forward, flow_backward, th = 2):\n",
        "        # assumes that flow forward is of shape (..., H, W, 2)\n",
        "        has_batch = False\n",
        "        if len(flow_forward.shape) == 3:\n",
        "            flow_forward, flow_backward = flow_forward[None], flow_backward[None]\n",
        "        else:\n",
        "            has_batch = True\n",
        "        H,W = flow_forward.shape[-3:-1]\n",
        "        th_n = 2 * th / max(H,W)\n",
        "        coords = torch.stack(torch.meshgrid(\n",
        "            torch.linspace(-1 + 1 / W, 1 - 1 / W, W),\n",
        "            torch.linspace(-1 + 1 / H, 1 - 1 / H, H), indexing = \"xy\"),\n",
        "                             dim = -1).to(flow_forward.device)\n",
        "        coords_fb = F.grid_sample(\n",
        "            flow_backward.permute(0, 3, 1, 2),\n",
        "            flow_forward,\n",
        "            align_corners=False, mode=\"bilinear\").permute(0, 2, 3, 1)\n",
        "        diff = (coords - coords_fb).norm(dim=-1)\n",
        "        in_th = (diff < th_n).float()\n",
        "        if not has_batch:\n",
        "            in_th = in_th[0]\n",
        "        return in_th\n",
        "\n",
        "    def to_pixel_coordinates(self, coords, H_A, W_A, H_B = None, W_B = None):\n",
        "        if coords.shape[-1] == 2:\n",
        "            return self._to_pixel_coordinates(coords, H_A, W_A)\n",
        "\n",
        "        if isinstance(coords, (list, tuple)):\n",
        "            kpts_A, kpts_B = coords[0], coords[1]\n",
        "        else:\n",
        "            kpts_A, kpts_B = coords[...,:2], coords[...,2:]\n",
        "        return self._to_pixel_coordinates(kpts_A, H_A, W_A), self._to_pixel_coordinates(kpts_B, H_B, W_B)\n",
        "\n",
        "    def _to_pixel_coordinates(self, coords, H, W):\n",
        "        kpts = torch.stack((W/2 * (coords[...,0]+1), H/2 * (coords[...,1]+1)),axis=-1)\n",
        "        return kpts\n",
        "\n",
        "    def to_normalized_coordinates(self, coords, H_A, W_A, H_B, W_B):\n",
        "        if isinstance(coords, (list, tuple)):\n",
        "            kpts_A, kpts_B = coords[0], coords[1]\n",
        "        else:\n",
        "            kpts_A, kpts_B = coords[...,:2], coords[...,2:]\n",
        "        kpts_A = torch.stack((2/W_A * kpts_A[...,0] - 1, 2/H_A * kpts_A[...,1] - 1),axis=-1)\n",
        "        kpts_B = torch.stack((2/W_B * kpts_B[...,0] - 1, 2/H_B * kpts_B[...,1] - 1),axis=-1)\n",
        "        return kpts_A, kpts_B\n",
        "\n",
        "    def match_keypoints(self, x_A, x_B, warp, certainty, return_tuple = True, return_inds = False):\n",
        "        x_A_to_B = F.grid_sample(warp[...,-2:].permute(2,0,1)[None], x_A[None,None], align_corners = False, mode = \"bilinear\")[0,:,0].mT\n",
        "        cert_A_to_B = F.grid_sample(certainty[None,None,...], x_A[None,None], align_corners = False, mode = \"bilinear\")[0,0,0]\n",
        "        D = torch.cdist(x_A_to_B, x_B)\n",
        "        inds_A, inds_B = torch.nonzero((D == D.min(dim=-1, keepdim = True).values) * (D == D.min(dim=-2, keepdim = True).values) * (cert_A_to_B[:,None] > self.sample_thresh), as_tuple = True)\n",
        "\n",
        "        if return_tuple:\n",
        "            if return_inds:\n",
        "                return inds_A, inds_B\n",
        "            else:\n",
        "                return x_A[inds_A], x_B[inds_B]\n",
        "        else:\n",
        "            if return_inds:\n",
        "                return torch.cat((inds_A, inds_B),dim=-1)\n",
        "            else:\n",
        "                return torch.cat((x_A[inds_A], x_B[inds_B]),dim=-1)\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def match(\n",
        "        self,\n",
        "        im_A_input,\n",
        "        im_B_input,\n",
        "        *args,\n",
        "        batched=False,\n",
        "        device=None,\n",
        "    ):\n",
        "        if device is None:\n",
        "            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        # Check if inputs are file paths or already loaded images\n",
        "        if isinstance(im_A_input, (str, os.PathLike)):\n",
        "            im_A = Image.open(im_A_input)\n",
        "            check_not_i16(im_A)\n",
        "            im_A = im_A.convert(\"RGB\")\n",
        "        else:\n",
        "            check_rgb(im_A_input)\n",
        "            im_A = im_A_input\n",
        "\n",
        "        if isinstance(im_B_input, (str, os.PathLike)):\n",
        "            im_B = Image.open(im_B_input)\n",
        "            check_not_i16(im_B)\n",
        "            im_B = im_B.convert(\"RGB\")\n",
        "        else:\n",
        "            check_rgb(im_B_input)\n",
        "            im_B = im_B_input\n",
        "\n",
        "        symmetric = self.symmetric\n",
        "        self.train(False)\n",
        "        with torch.no_grad():\n",
        "            if not batched:\n",
        "                b = 1\n",
        "                w, h = im_A.size\n",
        "                w2, h2 = im_B.size\n",
        "                # Get images in good format\n",
        "                ws = self.w_resized\n",
        "                hs = self.h_resized\n",
        "\n",
        "                test_transform = get_tuple_transform_ops(\n",
        "                    resize=(hs, ws), normalize=True, clahe=False\n",
        "                )\n",
        "                im_A, im_B = test_transform((im_A, im_B))\n",
        "                batch = {\"im_A\": im_A[None].to(device), \"im_B\": im_B[None].to(device)}\n",
        "            else:\n",
        "                b, c, h, w = im_A.shape\n",
        "                b, c, h2, w2 = im_B.shape\n",
        "                assert w == w2 and h == h2, \"For batched images we assume same size\"\n",
        "                batch = {\"im_A\": im_A.to(device), \"im_B\": im_B.to(device)}\n",
        "                if h != self.h_resized or self.w_resized != w:\n",
        "                    warn(\"Model resolution and batch resolution differ, may produce unexpected results\")\n",
        "                hs, ws = h, w\n",
        "            finest_scale = 1\n",
        "            # Run matcher\n",
        "            if symmetric:\n",
        "                corresps = self.forward_symmetric(batch)\n",
        "            else:\n",
        "                corresps = self.forward(batch, batched=True)\n",
        "\n",
        "            if self.upsample_preds:\n",
        "                hs, ws = self.upsample_res\n",
        "\n",
        "            if self.attenuate_cert:\n",
        "                low_res_certainty = F.interpolate(\n",
        "                    corresps[16][\"certainty\"], size=(hs, ws), align_corners=False, mode=\"bilinear\"\n",
        "                )\n",
        "                cert_clamp = 0\n",
        "                factor = 0.5\n",
        "                low_res_certainty = factor * low_res_certainty * (low_res_certainty < cert_clamp)\n",
        "\n",
        "            if self.upsample_preds:\n",
        "                finest_corresps = corresps[finest_scale]\n",
        "                torch.cuda.empty_cache()\n",
        "                test_transform = get_tuple_transform_ops(\n",
        "                    resize=(hs, ws), normalize=True\n",
        "                )\n",
        "                if isinstance(im_A_input, (str, os.PathLike)):\n",
        "                    im_A, im_B = test_transform(\n",
        "                        (Image.open(im_A_input).convert('RGB'), Image.open(im_B_input).convert('RGB')))\n",
        "                else:\n",
        "                    im_A, im_B = test_transform((im_A_input, im_B_input))\n",
        "\n",
        "                im_A, im_B = im_A[None].to(device), im_B[None].to(device)\n",
        "                scale_factor = math.sqrt(self.upsample_res[0] * self.upsample_res[1] / (self.w_resized * self.h_resized))\n",
        "                batch = {\"im_A\": im_A, \"im_B\": im_B, \"corresps\": finest_corresps}\n",
        "                if symmetric:\n",
        "                    corresps = self.forward_symmetric(batch, upsample=True, batched=True, scale_factor=scale_factor)\n",
        "                else:\n",
        "                    corresps = self.forward(batch, batched=True, upsample=True, scale_factor=scale_factor)\n",
        "\n",
        "            im_A_to_im_B = corresps[finest_scale][\"flow\"]\n",
        "            certainty = corresps[finest_scale][\"certainty\"] - (low_res_certainty if self.attenuate_cert else 0)\n",
        "            if finest_scale != 1:\n",
        "                im_A_to_im_B = F.interpolate(\n",
        "                    im_A_to_im_B, size=(hs, ws), align_corners=False, mode=\"bilinear\"\n",
        "                )\n",
        "                certainty = F.interpolate(\n",
        "                    certainty, size=(hs, ws), align_corners=False, mode=\"bilinear\"\n",
        "                )\n",
        "            im_A_to_im_B = im_A_to_im_B.permute(\n",
        "                0, 2, 3, 1\n",
        "            )\n",
        "            # Create im_A meshgrid\n",
        "            im_A_coords = torch.meshgrid(\n",
        "                (\n",
        "                    torch.linspace(-1 + 1 / hs, 1 - 1 / hs, hs, device=device),\n",
        "                    torch.linspace(-1 + 1 / ws, 1 - 1 / ws, ws, device=device),\n",
        "                ),\n",
        "                indexing='ij'\n",
        "            )\n",
        "            im_A_coords = torch.stack((im_A_coords[1], im_A_coords[0]))\n",
        "            im_A_coords = im_A_coords[None].expand(b, 2, hs, ws)\n",
        "            certainty = certainty.sigmoid()  # logits -> probs\n",
        "            im_A_coords = im_A_coords.permute(0, 2, 3, 1)\n",
        "            if (im_A_to_im_B.abs() > 1).any() and True:\n",
        "                wrong = (im_A_to_im_B.abs() > 1).sum(dim=-1) > 0\n",
        "                certainty[wrong[:, None]] = 0\n",
        "            im_A_to_im_B = torch.clamp(im_A_to_im_B, -1, 1)\n",
        "            if symmetric:\n",
        "                A_to_B, B_to_A = im_A_to_im_B.chunk(2)\n",
        "                q_warp = torch.cat((im_A_coords, A_to_B), dim=-1)\n",
        "                im_B_coords = im_A_coords\n",
        "                s_warp = torch.cat((B_to_A, im_B_coords), dim=-1)\n",
        "                warp = torch.cat((q_warp, s_warp), dim=2)\n",
        "                certainty = torch.cat(certainty.chunk(2), dim=3)\n",
        "            else:\n",
        "                warp = torch.cat((im_A_coords, im_A_to_im_B), dim=-1)\n",
        "            if batched:\n",
        "                return (\n",
        "                    warp,\n",
        "                    certainty[:, 0]\n",
        "                )\n",
        "            else:\n",
        "                return (\n",
        "                    warp[0],\n",
        "                    certainty[0, 0],\n",
        "                )\n",
        "\n",
        "    def visualize_warp(self, warp, certainty, im_A = None, im_B = None,\n",
        "                       im_A_path = None, im_B_path = None, device = \"cuda\", symmetric = True, save_path = None, unnormalize = False):\n",
        "        #assert symmetric == True, \"Currently assuming bidirectional warp, might update this if someone complains ;)\"\n",
        "        H,W2,_ = warp.shape\n",
        "        W = W2//2 if symmetric else W2\n",
        "        if im_A is None:\n",
        "            from PIL import Image\n",
        "            im_A, im_B = Image.open(im_A_path).convert(\"RGB\"), Image.open(im_B_path).convert(\"RGB\")\n",
        "        if not isinstance(im_A, torch.Tensor):\n",
        "            im_A = im_A.resize((W,H))\n",
        "            im_B = im_B.resize((W,H))\n",
        "            x_B = (torch.tensor(np.array(im_B)) / 255).to(device).permute(2, 0, 1)\n",
        "            if symmetric:\n",
        "                x_A = (torch.tensor(np.array(im_A)) / 255).to(device).permute(2, 0, 1)\n",
        "        else:\n",
        "            if symmetric:\n",
        "                x_A = im_A\n",
        "            x_B = im_B\n",
        "        im_A_transfer_rgb = F.grid_sample(\n",
        "        x_B[None], warp[:,:W, 2:][None], mode=\"bilinear\", align_corners=False\n",
        "        )[0]\n",
        "        if symmetric:\n",
        "            im_B_transfer_rgb = F.grid_sample(\n",
        "            x_A[None], warp[:, W:, :2][None], mode=\"bilinear\", align_corners=False\n",
        "            )[0]\n",
        "            warp_im = torch.cat((im_A_transfer_rgb,im_B_transfer_rgb),dim=2)\n",
        "            white_im = torch.ones((H,2*W),device=device)\n",
        "        else:\n",
        "            warp_im = im_A_transfer_rgb\n",
        "            white_im = torch.ones((H, W), device = device)\n",
        "        vis_im = certainty * warp_im + (1 - certainty) * white_im\n",
        "        if save_path is not None:\n",
        "            from romatch.utils import tensor_to_pil\n",
        "            tensor_to_pil(vis_im, unnormalize=unnormalize).save(save_path)\n",
        "        return vis_im\n",
        "\n",
        "\n",
        "def roma_model(resolution, upsample_preds, device = None, weights=None, dinov2_weights=None, amp_dtype: torch.dtype=torch.float16, **kwargs):\n",
        "    warnings.filterwarnings('ignore', category=UserWarning, message='TypedStorage is deprecated')\n",
        "    gp_dim = 512\n",
        "    feat_dim = 512\n",
        "    decoder_dim = gp_dim + feat_dim\n",
        "    cls_to_coord_res = 64\n",
        "    coordinate_decoder = TransformerDecoder(\n",
        "        nn.Sequential(*[Block(decoder_dim, 8, attn_class=MemEffAttention) for _ in range(5)]),\n",
        "        decoder_dim,\n",
        "        cls_to_coord_res**2 + 1,\n",
        "        is_classifier=True,\n",
        "        amp = True,\n",
        "        pos_enc = False,)\n",
        "    dw = True\n",
        "    hidden_blocks = 8\n",
        "    kernel_size = 5\n",
        "    displacement_emb = \"linear\"\n",
        "    disable_local_corr_grad = True\n",
        "\n",
        "    conv_refiner = nn.ModuleDict(\n",
        "        {\n",
        "            \"16\": ConvRefiner(\n",
        "                2 * 512+128+(2*7+1)**2,\n",
        "                2 * 512+128+(2*7+1)**2,\n",
        "                2 + 1,\n",
        "                kernel_size=kernel_size,\n",
        "                dw=dw,\n",
        "                hidden_blocks=hidden_blocks,\n",
        "                displacement_emb=displacement_emb,\n",
        "                displacement_emb_dim=128,\n",
        "                local_corr_radius = 7,\n",
        "                corr_in_other = True,\n",
        "                amp = True,\n",
        "                disable_local_corr_grad = disable_local_corr_grad,\n",
        "                bn_momentum = 0.01,\n",
        "            ),\n",
        "            \"8\": ConvRefiner(\n",
        "                2 * 512+64+(2*3+1)**2,\n",
        "                2 * 512+64+(2*3+1)**2,\n",
        "                2 + 1,\n",
        "                kernel_size=kernel_size,\n",
        "                dw=dw,\n",
        "                hidden_blocks=hidden_blocks,\n",
        "                displacement_emb=displacement_emb,\n",
        "                displacement_emb_dim=64,\n",
        "                local_corr_radius = 3,\n",
        "                corr_in_other = True,\n",
        "                amp = True,\n",
        "                disable_local_corr_grad = disable_local_corr_grad,\n",
        "                bn_momentum = 0.01,\n",
        "            ),\n",
        "            \"4\": ConvRefiner(\n",
        "                2 * 256+32+(2*2+1)**2,\n",
        "                2 * 256+32+(2*2+1)**2,\n",
        "                2 + 1,\n",
        "                kernel_size=kernel_size,\n",
        "                dw=dw,\n",
        "                hidden_blocks=hidden_blocks,\n",
        "                displacement_emb=displacement_emb,\n",
        "                displacement_emb_dim=32,\n",
        "                local_corr_radius = 2,\n",
        "                corr_in_other = True,\n",
        "                amp = True,\n",
        "                disable_local_corr_grad = disable_local_corr_grad,\n",
        "                bn_momentum = 0.01,\n",
        "            ),\n",
        "            \"2\": ConvRefiner(\n",
        "                2 * 64+16,\n",
        "                128+16,\n",
        "                2 + 1,\n",
        "                kernel_size=kernel_size,\n",
        "                dw=dw,\n",
        "                hidden_blocks=hidden_blocks,\n",
        "                displacement_emb=displacement_emb,\n",
        "                displacement_emb_dim=16,\n",
        "                amp = True,\n",
        "                disable_local_corr_grad = disable_local_corr_grad,\n",
        "                bn_momentum = 0.01,\n",
        "            ),\n",
        "            \"1\": ConvRefiner(\n",
        "                2 * 9 + 6,\n",
        "                24,\n",
        "                2 + 1,\n",
        "                kernel_size=kernel_size,\n",
        "                dw=dw,\n",
        "                hidden_blocks = hidden_blocks,\n",
        "                displacement_emb = displacement_emb,\n",
        "                displacement_emb_dim = 6,\n",
        "                amp = True,\n",
        "                disable_local_corr_grad = disable_local_corr_grad,\n",
        "                bn_momentum = 0.01,\n",
        "            ),\n",
        "        }\n",
        "    )\n",
        "    kernel_temperature = 0.2\n",
        "    learn_temperature = False\n",
        "    no_cov = True\n",
        "    kernel = CosKernel\n",
        "    only_attention = False\n",
        "    basis = \"fourier\"\n",
        "    gp16 = GP(\n",
        "        kernel,\n",
        "        T=kernel_temperature,\n",
        "        learn_temperature=learn_temperature,\n",
        "        only_attention=only_attention,\n",
        "        gp_dim=gp_dim,\n",
        "        basis=basis,\n",
        "        no_cov=no_cov,\n",
        "    )\n",
        "    gps = nn.ModuleDict({\"16\": gp16})\n",
        "    proj16 = nn.Sequential(nn.Conv2d(1024, 512, 1, 1), nn.BatchNorm2d(512))\n",
        "    proj8 = nn.Sequential(nn.Conv2d(512, 512, 1, 1), nn.BatchNorm2d(512))\n",
        "    proj4 = nn.Sequential(nn.Conv2d(256, 256, 1, 1), nn.BatchNorm2d(256))\n",
        "    proj2 = nn.Sequential(nn.Conv2d(128, 64, 1, 1), nn.BatchNorm2d(64))\n",
        "    proj1 = nn.Sequential(nn.Conv2d(64, 9, 1, 1), nn.BatchNorm2d(9))\n",
        "    proj = nn.ModuleDict({\n",
        "        \"16\": proj16,\n",
        "        \"8\": proj8,\n",
        "        \"4\": proj4,\n",
        "        \"2\": proj2,\n",
        "        \"1\": proj1,\n",
        "        })\n",
        "    displacement_dropout_p = 0.0\n",
        "    gm_warp_dropout_p = 0.0\n",
        "    decoder = Decoder(coordinate_decoder,\n",
        "                      gps,\n",
        "                      proj,\n",
        "                      conv_refiner,\n",
        "                      detach=True,\n",
        "                      scales=[\"16\", \"8\", \"4\", \"2\", \"1\"],\n",
        "                      displacement_dropout_p = displacement_dropout_p,\n",
        "                      gm_warp_dropout_p = gm_warp_dropout_p)\n",
        "\n",
        "    encoder = CNNandDinov2(\n",
        "        cnn_kwargs = dict(\n",
        "            pretrained=False,\n",
        "            amp = True),\n",
        "        amp = True,\n",
        "        use_vgg = True,\n",
        "        dinov2_weights = dinov2_weights,\n",
        "        amp_dtype=amp_dtype,\n",
        "    )\n",
        "    h,w = resolution\n",
        "    symmetric = True\n",
        "    attenuate_cert = True\n",
        "    sample_mode = \"threshold_balanced\"\n",
        "    matcher = RegressionMatcher(encoder, decoder, h=h, w=w, upsample_preds=upsample_preds,\n",
        "                                symmetric = symmetric, attenuate_cert = attenuate_cert, sample_mode = sample_mode, **kwargs).to(device)\n",
        "    matcher.load_state_dict(weights)\n",
        "    return matcher\n",
        "\n",
        "\n",
        "weight_urls = {\n",
        "    \"romatch\": {\n",
        "        \"outdoor\": \"https://github.com/Parskatt/storage/releases/download/roma/roma_outdoor.pth\",\n",
        "        \"indoor\": \"https://github.com/Parskatt/storage/releases/download/roma/roma_indoor.pth\",\n",
        "    },\n",
        "    \"tiny_roma_v1\": {\n",
        "        \"outdoor\": \"https://github.com/Parskatt/storage/releases/download/roma/tiny_roma_v1_outdoor.pth\",\n",
        "    },\n",
        "    \"dinov2\": \"https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_pretrain.pth\", #hopefully this doesnt change :D\n",
        "}\n",
        "\n",
        "\n",
        "def roma_outdoor(device, weights=None, dinov2_weights=None, coarse_res: Union[int,tuple[int,int]] = 560, upsample_res: Union[int,tuple[int,int]] = 864, amp_dtype: torch.dtype = torch.float16):\n",
        "    if isinstance(coarse_res, int):\n",
        "        coarse_res = (coarse_res, coarse_res)\n",
        "    if isinstance(upsample_res, int):\n",
        "        upsample_res = (upsample_res, upsample_res)\n",
        "\n",
        "    if str(device) == 'cpu':\n",
        "        amp_dtype = torch.float32\n",
        "\n",
        "    assert coarse_res[0] % 14 == 0, \"Needs to be multiple of 14 for backbone\"\n",
        "    assert coarse_res[1] % 14 == 0, \"Needs to be multiple of 14 for backbone\"\n",
        "\n",
        "    if weights is None:\n",
        "        weights = torch.hub.load_state_dict_from_url(weight_urls[\"romatch\"][\"outdoor\"],\n",
        "                                                     map_location=device)\n",
        "    if dinov2_weights is None:\n",
        "        dinov2_weights = torch.hub.load_state_dict_from_url(weight_urls[\"dinov2\"],\n",
        "                                                     map_location=device)\n",
        "    model = roma_model(resolution=coarse_res, upsample_preds=True,\n",
        "               weights=weights,dinov2_weights = dinov2_weights,device=device, amp_dtype=amp_dtype)\n",
        "    model.upsample_res = upsample_res\n",
        "    print(f\"Using coarse resolution {coarse_res}, and upsample res {model.upsample_res}\")\n",
        "    return model\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "    # Speed up PyTorch's conv algorithms if input sizes are consistent\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    TEST_DIR = \"/content/dataset/test_images\"\n",
        "    TEST_CSV = \"/content/dataset/test.csv\"\n",
        "    SUBMISSION_CSV = \"/content/submission.csv\"\n",
        "    WEIGHTS_PTH = \"/content/roma_full_weights.pth\"\n",
        "\n",
        "    if not os.path.isfile(TEST_CSV) or not os.path.isdir(TEST_DIR):\n",
        "        print(f\"Test Files not found!\")\n",
        "        return\n",
        "\n",
        "    test_rows = []\n",
        "    with open(TEST_CSV, \"r\", newline=\"\") as f:\n",
        "        reader = csv.DictReader(f)\n",
        "        for row in reader:\n",
        "            sample_id = row[\"sample_id\"]\n",
        "            batch_id  = row[\"batch_id\"]\n",
        "            im1_id    = row[\"image_1_id\"]\n",
        "            im2_id    = row[\"image_2_id\"]\n",
        "            test_rows.append((sample_id, batch_id, im1_id, im2_id))\n",
        "    print(f\"Found {len(test_rows)} pairs in {TEST_CSV}.\")\n",
        "\n",
        "    roma_model = roma_outdoor(device=device)\n",
        "\n",
        "    results = []\n",
        "    start_inference = time.time()\n",
        "\n",
        "    try:\n",
        "        # Use inference_mode for speed\n",
        "        with torch.inference_mode():\n",
        "            with tqdm(total=len(test_rows), desc=\"Estimating F on test pairs\") as pbar:\n",
        "                for (sample_id, scene_name, im1_id, im2_id) in test_rows:\n",
        "                    try:\n",
        "                        img1_path = os.path.join(TEST_DIR, scene_name, im1_id + \".jpg\")\n",
        "                        img2_path = os.path.join(TEST_DIR, scene_name, im2_id + \".jpg\")\n",
        "\n",
        "                        # Check existence\n",
        "                        if not (os.path.isfile(img1_path) and os.path.isfile(img2_path)):\n",
        "                            F_est = np.zeros((3,3), dtype=np.float64)\n",
        "                        else:\n",
        "                            # Load sizes\n",
        "                            W_A, H_A = Image.open(img1_path).size\n",
        "                            W_B, H_B = Image.open(img2_path).size\n",
        "\n",
        "                            # Match with RoMa\n",
        "                            warp, certainty = roma_model.match(img1_path, img2_path, device=device)\n",
        "\n",
        "                            # Sample\n",
        "                            matches, c = roma_model.sample(warp, certainty)\n",
        "\n",
        "                            # Convert to pixel coords\n",
        "                            kpts1, kpts2 = roma_model.to_pixel_coordinates(matches, H_A, W_A, H_B, W_B)\n",
        "                            kpts1_np = kpts1.cpu().numpy()\n",
        "                            kpts2_np = kpts2.cpu().numpy()\n",
        "\n",
        "                            if kpts1_np.shape[0] < 8:\n",
        "                                # Not enough matches\n",
        "                                F_est = np.zeros((3,3), dtype=np.float64)\n",
        "\n",
        "                            else:\n",
        "                                # 4) Estimate F\n",
        "                                try:\n",
        "                                    F_est, mask = cv2.findFundamentalMat(\n",
        "                                        kpts1_np,\n",
        "                                        kpts2_np,\n",
        "                                        ransacReprojThreshold=0.7, # Changed from 0.2 after fine-tuning and optimization\n",
        "                                        method=cv2.USAC_MAGSAC,\n",
        "                                        confidence=0.999999,\n",
        "                                        maxIters=10000\n",
        "                                    )\n",
        "                                except cv2.error:\n",
        "                                    F_est = None\n",
        "                                if F_est is None or F_est.shape != (3,3):\n",
        "                                    F_est = np.zeros((3,3), dtype=np.float64)\n",
        "\n",
        "                        # Flatten for submission\n",
        "                        F_str = \" \".join(f\"{val:e}\" for val in F_est.flatten())\n",
        "                        results.append((sample_id, F_str))\n",
        "\n",
        "                    except Exception as e: # Usually happens when VRAM is full\n",
        "                        print(f\"Error processing pair {sample_id}: {e}\")\n",
        "                        F_est = np.zeros((3,3), dtype=np.float64)\n",
        "                        F_str = \" \".join(f\"{val:e}\" for val in F_est.flatten())\n",
        "                        results.append((sample_id, F_str))\n",
        "\n",
        "                    finally:\n",
        "                        pbar.update(1)\n",
        "\n",
        "        end_inference = time.time()\n",
        "        print(f\"Done estimating F for all pairs in {end_inference - start_inference:.2f} seconds.\")\n",
        "\n",
        "        # Save F predictions and model weights\n",
        "\n",
        "        with open(SUBMISSION_CSV, \"w\", newline=\"\") as fout:\n",
        "            writer = csv.writer(fout)\n",
        "            writer.writerow([\"sample_id\", \"fundamental_matrix\"])\n",
        "            for sample_id, F_str in results:\n",
        "                writer.writerow([sample_id, F_str])\n",
        "\n",
        "        print(f\"Wrote {len(results)} rows to {SUBMISSION_CSV}.\")\n",
        "        torch.save(roma_model.state_dict(), WEIGHTS_PTH)\n",
        "        print('Saved mode weights')\n",
        "        files.download(SUBMISSION_CSV) # Remove if code is not run in Colab\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
